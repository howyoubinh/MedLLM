---
system:
  data_path: "./PMC-LLaMA/data"
  model_path: "./PMC-LLaMA/models" # The path to the ollama and inference models
  cache_path: "./PMC-LLaMA/cached_data"
  results_path: "./PMC-LLaMA/results"
  evaluation_path: "./PMC-LLaMA/eval"

experiment:
  name: 'Inference_on_one'

dataset:
  name: 'medqa' # Current datasets: 'medqa','medmcqa'
  set: 'test'   # 
  custom_dataset_path: '' # leave empty unless using custom dataset. 
                          # If using this, make sure you update the name and set

augmentation:
  do_augment: False
  augmentations: None # ['synonyms','shuffle','paraphrase','expand_context']
  model: 'llama3.2' # models should be loaded with ollama
  preamble: "Below is an instruction that describes a task, paired with an input that provides further context. "
  instruction: "You're a doctor, kindly address the medical queries according to the patient's account. Analyze the question by option and answer with the best option."
  parallel: True
  num_workers: 4
  overwrite: False

inference:
  do_inference: True
  model: 'axiong/PMC_LLaMA_13B' # This is the default model
  precision: '16' # 'b16': torch.bfloat16, '16': torch.float16, '32': torch.float32
  num_samples: -1
  batch_size: 1

evaluation:
  do_evaluation: False
